Json Summarizer

import json
import random
import spacy
import logging
import pandas as pd
from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_fscore_support
from spacy.gold import GoldParse
from spacy.scorer import Scorer
from sklearn.metrics import accuracy_score

nlp = spacy.blank('en')

def change_to_spacy(filepath):
    
    try:
        training_data = []
        lines=[]
        file = open(filepath, 'r', encoding='UTF8')
        lines = file.readlines()

        for line in lines:
            data = json.loads(line)
            entities = [] 

            sno = len(data['samples'])

            for item in range(sno):
                entities = [] 
                diction = data['samples'][item]
                text = diction['document']
                notes = diction['annotation']

                eno = len(notes['entities'])

                for i in range(eno):
                    note = notes['entities'][i]
                    labels = note['label']
                    start = note['start']
                    end = note['end']

                    if not isinstance(labels, list):
                        labels = [labels]

                    for label in labels:
                        entities.append((start, end, label))

                training_data.append((text, {"entities" : entities}))
        
        return training_data
    
    except Exception as e:
        logging.exception("Unable to process " + filepath + "\n" + "error = " + str(e))
        return None

train_data = change_to_spacy('CVFINAL.json')

def train_model(train_data):
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last = True)
        
    for _, annotation in train_data:
        for ent in annotation['entities']:
            ner.add_label(ent[2])
            
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    
    with nlp.disable_pipes(*other_pipes):
        optimizer = nlp.begin_training()
        
        for itn in range(25):
            print("Starting iteration " + str(itn))
            random.shuffle(train_data)
            losses = {}
            
            for text, annotations in train_data:
                
                try:
                    nlp.update(
                        [text], 
                        [annotations],
                        drop = 0.2,
                        sgd = optimizer,
                        losses = losses)
                except Exception as e:
                    pass
            
            print(losses)

train_model(train_data)

nlp.to_disk('nlpdesc_CV_1_model')

nlpdesc_model = spacy.load('nlpdesc_model')
nlpdesc_CV_model = spacy.load('nlpdesc_CV_1_model')

import sys, fitz

fname = 'job.pdf'
doc = fitz.open(fname)
text = ""

for page in doc:
    text = text + str(page.getText())
    
tx = " ".join(text.split('\n'))
# print(tx)

doc = nlpdesc_model(tx)

for ent in doc.ents:
    print(f'{ent.label_.upper():{30}} - {ent.text}')

examples = change_to_spacy("testdata.json")
tp=0
tr=0
tf=0

ta=0
c=0        
for text,annot in examples:
    doc_to_test=nlpdesc_CV_model(text)
    d={}
    for ent in doc_to_test.ents:
        d[ent.label_]=[]
    for ent in doc_to_test.ents:
#         print(ent.label_)
        d[ent.label_].append(ent.text)

#     for i in set(d.keys()):
#         f.write("\n\n")
#         f.write(i +":"+"\n")
#         for j in set(d[i]):
#             f.write(j.replace('\n','')+"\n")
    d={}
    for ent in doc_to_test.ents:
        d[ent.label_]=[0,0,0,0,0,0]
    for ent in doc_to_test.ents:
        doc_gold_text= nlpdesc_CV_model.make_doc(text)
        gold = GoldParse(doc_gold_text, entities=annot.get("entities"))
        y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]
        y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  
        if(d[ent.label_][0]==0):
                #f.write("For Entity "+ent.label_+"\n")   
                #f.write(classification_report(y_true, y_pred)+"\n")
            (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')
            a=accuracy_score(y_true,y_pred)
            d[ent.label_][0]=1
            d[ent.label_][1]+=p
            d[ent.label_][2]+=r
            d[ent.label_][3]+=f
            d[ent.label_][4]+=a
            d[ent.label_][5]+=1
    c+=1
for i in d:
    print("\n For Entity "+i+"\n")
    print("Accuracy : "+str((d[i][4]/d[i][5])*100)+"%")
    print("Precision : "+str(d[i][1]/d[i][5]))
    print("Recall : "+str(d[i][2]/d[i][5]))
    print("F-score : "+str(d[i][3]/d[i][5]))

examples = change_to_spacy("finaldata.json")
tp=0
tr=0
tf=0

ta=0
c=0        
for text,annot in examples:
    doc_to_test=nlpdesc_model(text)
    d={}
    for ent in doc_to_test.ents:
        d[ent.label_]=[]
    for ent in doc_to_test.ents:
#         print(ent.label_)
        d[ent.label_].append(ent.text)

#     for i in set(d.keys()):
#         f.write("\n\n")
#         f.write(i +":"+"\n")
#         for j in set(d[i]):
#             f.write(j.replace('\n','')+"\n")
    d={}
    for ent in doc_to_test.ents:
        d[ent.label_]=[0,0,0,0,0,0]
    for ent in doc_to_test.ents:
        doc_gold_text= nlpdesc_model.make_doc(text)
        gold = GoldParse(doc_gold_text, entities=annot.get("entities"))
        y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]
        y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  
        if(d[ent.label_][0]==0):
                #f.write("For Entity "+ent.label_+"\n")   
                #f.write(classification_report(y_true, y_pred)+"\n")
            (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')
            a=accuracy_score(y_true,y_pred)
            d[ent.label_][0]=1
            d[ent.label_][1]+=p
            d[ent.label_][2]+=r
            d[ent.label_][3]+=f
            d[ent.label_][4]+=a
            d[ent.label_][5]+=1
    c+=1
for i in d:
    print("\n For Entity "+i+"\n")
    print("Accuracy : "+str((d[i][4]/d[i][5])*100)+"%")
    print("Precision : "+str(d[i][1]/d[i][5]))
    print("Recall : "+str(d[i][2]/d[i][5]))
    print("F-score : "+str(d[i][3]/d[i][5]))